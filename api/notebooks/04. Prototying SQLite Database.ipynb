{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prototying SQLite Database**\n",
    "\n",
    "I want the API to be capable of running a pre-processing pipeline on a user's uploaded `conversations.json`. That pre-processing pipeline will populate a SQLite database w/ data that can then be used by the FastAPI!\n",
    "\n",
    "Within this notebook, I'm going to prototype that database's initialization. This code will eventually be moved into a singular pipeline that can be called by the FastAPI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import math\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "\n",
    "# Project imports\n",
    "import utils.data_parsing as data_utils\n",
    "import utils.llm_enrichment as llm_utils\n",
    "import utils.openai as openai_utils\n",
    "import utils.clusters as cluster_utils\n",
    "import utils.pipelines as pipeline_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterizing Pipeline\n",
    "\n",
    "Below, we're going to parameterize the overall pipeline a bit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate where the conversations.json file is located\n",
    "conversations_json_file_path = \"data/export/conversations.json\"\n",
    "\n",
    "# Load in the conversations.json file\n",
    "with open(conversations_json_file_path) as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "# Declare a constant indicating the number of characters we'll include in the context\n",
    "max_n_chars_in_conversation_context = 4_000\n",
    "\n",
    "# TODO: I need to remove this eventually, but: I'm subsetting the conversations for testing purposes\n",
    "conversations = conversations[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tables\n",
    "\n",
    "First, we'll need to initialize a couple of different tables. Before doing that, I'll create a database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the database\n",
    "db_conn = sqlite3.connect(\"data/test_db.db\")\n",
    "\n",
    "# Create a cursor object\n",
    "db_cursor = db_conn.cursor()\n",
    "\n",
    "# Drop the conversations + clusters tables if they exist\n",
    "db_cursor.execute(\"DROP TABLE IF EXISTS conversations\")\n",
    "db_cursor.execute(\"DROP TABLE IF EXISTS clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`conversations`**\n",
    "\n",
    "This table will store information about each ChatGPT conversation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that'll generate the conversations table\n",
    "create_conversations_table_query = \"\"\"\n",
    "CREATE TABLE conversations (\n",
    "    conversation_id TEXT PRIMARY_KEY, -- This is the unique identifier for the conversation\n",
    "    title TEXT NOT NULL, -- This is the title of the conversation\n",
    "    create_time INTEGER, -- This is a Unix timestamp indicating when the conversation was created\n",
    "    default_model_slug TEXT, -- This is the model that was used to generate the conversation\n",
    "    raw_messages_data TEXT, -- This is a JSON string containing the entire conversation's raw messages data\n",
    "    messages_markdown TEXT, -- This is a Markdown string containing the entire conversation\n",
    "    summary TEXT, -- This is a summary of the conversation\n",
    "    tags TEXT, -- This is a JSON string containing a list of tags associated with the conversation\n",
    "    embedding BLOB, -- This is a binary large object containing the conversation's embedding\n",
    "    umap_x REAL, -- This is the UMAP x-coordinate of the conversation's embedding\n",
    "    umap_y REAL -- This is the UMAP y-coordinate of the conversation's embedding\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "db_cursor.execute(create_conversations_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`clusters`**\n",
    "\n",
    "Next up: we're going to create a table called `clusters`. Whenever the user creates some semantic clusters for their data, it'll be stored here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that'll generate the clusters table\n",
    "create_clusters_table_query = \"\"\"\n",
    "CREATE TABLE clusters (\n",
    "    cluster_solution_id TEXT,\n",
    "    cluster_id TEXT,\n",
    "    conversation_ids TEXT, -- This is a JSON string containing a list of the conversation IDs that belong to this cluster\n",
    "    centroid_conversation_ids TEXT, -- This is a JSON string containing a list of the conversation IDs that are the centroids of this cluster\n",
    "    centroid_embedding BLOB, -- This is a binary large object containing the centroid embedding of the cluster\n",
    "    cluster_size INTEGER, -- The number of conversations in the cluster\n",
    "    cluster_label TEXT, -- The label of the cluster\n",
    "    cluster_description TEXT, -- A description of the cluster\n",
    "    tag_counts TEXT, -- This is a JSON string containing the counts of particular tags in the cluster\n",
    "    mean_cosine_similarity REAL, -- The mean cosine similarity between points and centroid\n",
    "    cluster_radius REAL, -- The maximum distance from any point to centroid\n",
    "    silhouette_score REAL, -- The silhouette score for this cluster\n",
    "    centroid_umap_x REAL, -- The UMAP x-coordinate of the centroid\n",
    "    centroid_umap_y REAL, -- The UMAP y-coordinate of the centroid\n",
    "    PRIMARY KEY (cluster_solution_id, cluster_id)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "db_cursor.execute(create_clusters_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating Tables\n",
    "\n",
    "Next up: I'm going to populate the tables a bit!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Initial Data to **`conversations`**\n",
    "\n",
    "Below, I'll populate the `conversations` table with data from the `conversations.json`. I'll start by extracting the necessary data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data to insert into the conversations table\n",
    "conversations_data_to_insert = []\n",
    "for conversation in tqdm(\n",
    "    iterable=conversations,\n",
    "    desc=\"Processing conversations\",\n",
    "):\n",
    "\n",
    "    # Extract a DataFrame representation of the messages in this conversation\n",
    "    messasges_df = data_utils.extract_longest_conversation_df(\n",
    "        mapping=conversation.get(\"mapping\", {})\n",
    "    )\n",
    "\n",
    "    # Generate the data to insert\n",
    "    cur_conversation_data = {\n",
    "        \"conversation_id\": conversation.get(\"conversation_id\"),\n",
    "        \"title\": conversation.get(\"title\", \"UNTITLED CONVERSATION\"),\n",
    "        \"create_time\": conversation.get(\"create_time\"),\n",
    "        \"default_model_slug\": conversation.get(\"default_model_slug\"),\n",
    "        \"raw_messages_data\": messasges_df.to_json(),\n",
    "        \"messages_markdown\": data_utils.extract_simple_conversation_markdown(\n",
    "            conversation_df=messasges_df\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Add the data to the list\n",
    "    conversations_data_to_insert.append(cur_conversation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll add all of this data to the SQLite database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the data into the conversations table\n",
    "insert_conversations_query = \"\"\"\n",
    "INSERT INTO conversations (\n",
    "    conversation_id,\n",
    "    title,\n",
    "    create_time,\n",
    "    default_model_slug,\n",
    "    raw_messages_data,\n",
    "    messages_markdown\n",
    ") VALUES (\n",
    "    :conversation_id,\n",
    "    :title,\n",
    "    :create_time,\n",
    "    :default_model_slug,\n",
    "    :raw_messages_data,\n",
    "    :messages_markdown\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "db_cursor.executemany(insert_conversations_query, conversations_data_to_insert)\n",
    "\n",
    "# Commit the changes\n",
    "db_conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching Data in `conversations` Table\n",
    "\n",
    "Next up, we'll want to do a little \"data enrichment\" for the conversations table. During this step, we'll add LLM-generated summaries & tags, as well as embeddings.\n",
    "\n",
    "I'll start by running the LLM enrichment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the conversations with LLM summaries & tags\n",
    "llm_enriched_conversations_df = llm_utils.enrich_conversations_with_summaries_and_tags(\n",
    "    conversations_df=pd.DataFrame(conversations_data_to_insert),\n",
    "    max_chars_per_conversation_context=max_n_chars_in_conversation_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll add the `summary` and `tags` to the database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data to insert into the conversations table\n",
    "conversations_data_to_insert = [\n",
    "    {\n",
    "        \"conversation_id\": row.conversation_id,\n",
    "        \"summary\": row.summary,\n",
    "        \"tags\": json.dumps(row.tags),\n",
    "    }\n",
    "    for row in llm_enriched_conversations_df.itertuples()\n",
    "]\n",
    "\n",
    "# Insert the data into the conversations table\n",
    "insert_conversations_query = \"\"\"\n",
    "UPDATE conversations\n",
    "SET\n",
    "    summary = :summary,\n",
    "    tags = :tags\n",
    "WHERE\n",
    "    conversation_id = :conversation_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "db_cursor.executemany(insert_conversations_query, conversations_data_to_insert)\n",
    "\n",
    "# Commit the changes\n",
    "db_conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: I'm going to embed the conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the embeddings for the conversations\n",
    "embs = openai_utils.generate_embeddings_for_texts(\n",
    "    text_list=[\n",
    "        f\"{row.title}\\nTags: {', '.join(row.tags)}\\nSummary: {row.summary}\\nConversation: {row.messages_markdown[:max_n_chars_in_conversation_context]}\"\n",
    "        for row in llm_enriched_conversations_df.itertuples()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a copy of the enriched conversations DataFrame\n",
    "conversation_emb_df = llm_enriched_conversations_df.copy()[[\"conversation_id\"]]\n",
    "\n",
    "# Add the embeddings to the DataFrame\n",
    "conversation_emb_df[\"embedding\"] = embs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll generate a UMAP model for the embeddings, and use it to generate UMAP projections of the embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I should figure out how to optimize this to go faster\n",
    "# Initialize and fit a UMAP model on the embeddings\n",
    "umap_model = UMAP(n_components=2)\n",
    "\n",
    "# Add the UMAP coordinates to the DataFrame\n",
    "conversation_emb_df[[\"umap_x\", \"umap_y\"]] = umap_model.fit_transform(\n",
    "    np.stack(conversation_emb_df.embedding)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conversations embedded, I can add them to the database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of floats to BLOB before storing\n",
    "conversations_data_to_insert = [\n",
    "    {\n",
    "        \"conversation_id\": row.conversation_id,\n",
    "        \"embedding\": np.array(\n",
    "            row.embedding, dtype=np.float32\n",
    "        ).tobytes(),  # Convert the list of floats to a BLOB\n",
    "        \"umap_x\": row.umap_x,\n",
    "        \"umap_y\": row.umap_y,\n",
    "    }\n",
    "    for row in conversation_emb_df.itertuples()\n",
    "]\n",
    "\n",
    "# Update the conversations table\n",
    "insert_conversations_query = \"\"\"\n",
    "UPDATE conversations\n",
    "SET\n",
    "    embedding = :embedding,\n",
    "    umap_x = :umap_x,\n",
    "    umap_y = :umap_y\n",
    "WHERE\n",
    "    conversation_id = :conversation_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "db_cursor.executemany(insert_conversations_query, conversations_data_to_insert)\n",
    "\n",
    "# Commit the changes\n",
    "db_conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating the **`clusters`** Table\n",
    "\n",
    "The application will allow users to create their own clustering solutions; still, though, to get them started, we'll create a solution based on the number of conversations they've got.\n",
    "\n",
    "First, we'll cluster the conversations and determine some metrics about those clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out a better default for determining the number of clusters\n",
    "n_clusters = min(math.ceil(math.sqrt(len(conversation_emb_df))), 24)\n",
    "\n",
    "# Cluster the conversations\n",
    "clusters_df = cluster_utils.cluster_conversations(\n",
    "    conversation_embs_df=conversation_emb_df.merge(\n",
    "        llm_enriched_conversations_df[[\"conversation_id\", \"tags\"]],\n",
    "        on=\"conversation_id\",\n",
    "    ),\n",
    "    n_clusters=n_clusters,\n",
    ")\n",
    "\n",
    "# Calculate some cluster metrics\n",
    "cluster_metrics_df = cluster_utils.calculate_cluster_metrics(clusters_df)\n",
    "\n",
    "# Add UMAP coordinates to the cluster_metrics_df\n",
    "cluster_metrics_df[[\"umap_x\", \"umap_y\"]] = umap_model.fit_transform(\n",
    "    np.stack(cluster_metrics_df.embedding_centroid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: I'm going to use LLMs to label these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LLM labeling for each of the conversation clusters\n",
    "labeled_clusters_df = llm_utils.label_conversation_clusters(\n",
    "    conversations_df=llm_enriched_conversations_df,\n",
    "    cluster_metrics_df=cluster_metrics_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll populate the `clusters` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cluster data into a format suitable for SQLite insertion\n",
    "clusters_data_to_insert = []\n",
    "for row in labeled_clusters_df.itertuples():\n",
    "    clusters_data_to_insert.append(\n",
    "        {\n",
    "            \"cluster_solution_id\": f\"kmeans_{n_clusters}\",\n",
    "            \"cluster_id\": row.cluster,\n",
    "            \"conversation_ids\": json.dumps(row.all_conversation_ids),\n",
    "            \"centroid_conversation_ids\": json.dumps(row.centroid_conversation_ids),\n",
    "            \"centroid_embedding\": np.array(\n",
    "                row.embedding_centroid, dtype=np.float32\n",
    "            ).tobytes(),\n",
    "            \"cluster_size\": row.n_conversations,\n",
    "            \"cluster_label\": row.cluster_label,\n",
    "            \"cluster_description\": row.cluster_description,\n",
    "            \"tag_counts\": json.dumps(row.tag_counts),\n",
    "            \"mean_cosine_similarity\": row.mean_cosine_similarity,\n",
    "            \"cluster_radius\": row.cluster_radius,\n",
    "            \"silhouette_score\": row.silhouette_score,\n",
    "            \"centroid_umap_x\": row.umap_x,\n",
    "            \"centroid_umap_y\": row.umap_y,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Insert the data into the clusters table\n",
    "insert_clusters_query = \"\"\"\n",
    "INSERT INTO clusters (\n",
    "    cluster_solution_id,\n",
    "    cluster_id,\n",
    "    conversation_ids,\n",
    "    centroid_conversation_ids,\n",
    "    centroid_embedding,\n",
    "    cluster_size,\n",
    "    cluster_label,\n",
    "    cluster_description,\n",
    "    tag_counts,\n",
    "    mean_cosine_similarity,\n",
    "    cluster_radius,\n",
    "    silhouette_score,\n",
    "    centroid_umap_x,\n",
    "    centroid_umap_y\n",
    ") VALUES (\n",
    "    :cluster_solution_id,\n",
    "    :cluster_id,\n",
    "    :conversation_ids,\n",
    "    :centroid_conversation_ids,\n",
    "    :centroid_embedding,\n",
    "    :cluster_size,\n",
    "    :cluster_label,\n",
    "    :cluster_description,\n",
    "    :tag_counts,\n",
    "    :mean_cosine_similarity,\n",
    "    :cluster_radius,\n",
    "    :silhouette_score,\n",
    "    :centroid_umap_x,\n",
    "    :centroid_umap_y\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "db_cursor.executemany(insert_clusters_query, clusters_data_to_insert)\n",
    "\n",
    "# Commit the changes\n",
    "db_conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalizing Pipeline\n",
    "I've combined all of the above code into a single method, which I'll test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate where the conversations.json file is located\n",
    "conversations_json_file_path = \"data/export/conversations.json\"\n",
    "\n",
    "# Load in the conversations.json file\n",
    "with open(conversations_json_file_path) as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "pipeline_utils.preprocess_conversation_data(\n",
    "    conversations=conversations,\n",
    "    db_path=\"data/parsed_export_data.db\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
