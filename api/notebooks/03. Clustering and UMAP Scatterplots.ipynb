{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clustering and UMAP Scatterplots**\n",
    "In this notebook, I'm going to cluster the conversation embeddings & visualize them on a super simple Plotly scatterplot! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import json\n",
    "\n",
    "# Third-party import statements\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "import utils.openai as openai_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "First, I'm going to load in the embeddings data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the folder path for the export data\n",
    "parsed_export_data = f\"./data/parsed_export\"\n",
    "\n",
    "# Load the conversation embeddings DataFrame\n",
    "conversation_embs_df = pd.read_parquet(\n",
    "    f\"{parsed_export_data}/conversation_embeddings.parquet\"\n",
    ")\n",
    "\n",
    "# Load the summarized_conversations DataFrame\n",
    "summarized_conversations_df = pd.read_json(\n",
    "    f\"{parsed_export_data}/summarized_conversations.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Embeddings\n",
    "First, I'm going to use `MiniBatchKMeans` to cluster the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit MiniBatchKMeans\n",
    "n_clusters = 100\n",
    "\n",
    "# Convert embeddings from object to numpy array\n",
    "embeddings_array = np.stack(conversation_embs_df[\"embedding\"].values)\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    batch_size=1024,  # Adjust batch size as needed\n",
    "    random_state=42,\n",
    ")\n",
    "cluster_labels = minibatch_kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "conversation_embs_df[\"cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Cluster Metrics\n",
    "Next up, I'm going to calculate some metrics about each of the conversation clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of centroid documents to find per cluster\n",
    "n_centroid_docs = 5\n",
    "max_n_tags_per_cluster = 10\n",
    "\n",
    "# Calculate distances from each point to its cluster centroid\n",
    "distances_to_centroid = minibatch_kmeans.transform(embeddings_array)\n",
    "\n",
    "# Get cluster centroids\n",
    "cluster_centroids = minibatch_kmeans.cluster_centers_\n",
    "\n",
    "# Create a DataFrame to store cluster info\n",
    "conversation_cluster_df = pd.DataFrame()\n",
    "conversation_cluster_df[\"cluster\"] = range(n_clusters)\n",
    "conversation_cluster_df[\"centroid_conversation_ids\"] = [[] for _ in range(n_clusters)]\n",
    "conversation_cluster_df[\"all_conversation_ids\"] = [[] for _ in range(n_clusters)]\n",
    "conversation_cluster_df[\"n_conversations\"] = 0\n",
    "conversation_cluster_df[\"embedding_centroid\"] = list(cluster_centroids)\n",
    "conversation_cluster_df[\"tag_counts\"] = [dict() for _ in range(n_clusters)]\n",
    "\n",
    "# For each cluster, find points closest to centroid and calculate metrics\n",
    "for cluster_id in tqdm(range(n_clusters)):\n",
    "    # Get indices of points in this cluster\n",
    "    cluster_mask = conversation_embs_df[\"cluster\"] == cluster_id\n",
    "\n",
    "    # Get all conversation IDs for this cluster\n",
    "    all_conv_ids = conversation_embs_df[cluster_mask][\"conversation_id\"].tolist()\n",
    "    conversation_cluster_df.at[cluster_id, \"all_conversation_ids\"] = all_conv_ids\n",
    "    conversation_cluster_df.at[cluster_id, \"n_conversations\"] = len(all_conv_ids)\n",
    "\n",
    "    # Get distances for points in this cluster\n",
    "    cluster_distances = distances_to_centroid[cluster_mask, cluster_id]\n",
    "\n",
    "    # Get indices of n_centroid_docs closest points\n",
    "    closest_indices = np.argsort(cluster_distances)[:n_centroid_docs]\n",
    "\n",
    "    # Get conversation IDs for these points\n",
    "    closest_conv_ids = (\n",
    "        conversation_embs_df[cluster_mask]\n",
    "        .iloc[closest_indices][\"conversation_id\"]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Store centroid conversation IDs\n",
    "    conversation_cluster_df.at[cluster_id, \"centroid_conversation_ids\"] = (\n",
    "        closest_conv_ids\n",
    "    )\n",
    "\n",
    "    # Calculate mean cosine similarity between points and centroid\n",
    "    cluster_embeddings = embeddings_array[cluster_mask]\n",
    "    centroid = cluster_centroids[cluster_id].reshape(1, -1)\n",
    "    similarities = cosine_similarity(cluster_embeddings, centroid)\n",
    "    conversation_cluster_df.at[cluster_id, \"mean_cosine_similarity\"] = (\n",
    "        similarities.mean()\n",
    "    )\n",
    "\n",
    "    # Get tag counts for this cluster\n",
    "    tags = summarized_conversations_df[\n",
    "        summarized_conversations_df[\"conversation_id\"].isin(all_conv_ids)\n",
    "    ][\"tags\"].explode()\n",
    "    tag_counts = (\n",
    "        tags.value_counts().head(max_n_tags_per_cluster).to_dict()\n",
    "    )  # Limiting to top 10 tags\n",
    "    conversation_cluster_df.at[cluster_id, \"tag_counts\"] = tag_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_cluster_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Clusters\n",
    "Next: I'll use ChatGPT to label each of the clusters. \n",
    "\n",
    "I'll start by defining a system prompt + Pydantic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt\n",
    "system_prompt = \"\"\"\n",
    "You're an intelligent AI assistant who likes responding in JSON.\n",
    "\n",
    "The user will provide you with a list of conversation summaries from a cluster of related conversations.\n",
    "Your task is to analyze these summaries and identify the common themes and topics that unite them.\n",
    "\n",
    "You'll provide:\n",
    "1. A brief, descriptive title for the cluster that captures its main theme\n",
    "2. A 1-2 sentence description explaining what types of conversations are in this cluster and what unites them\n",
    "\n",
    "Please ensure your response is concise but informative, focusing on the key patterns that emerge from the conversation cluster.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define a \"ConversationClusterSummary\" Pydantic model that will be used to validate the AI's response\n",
    "class ConversationClusterSummary(BaseModel):\n",
    "    title: str = Field(\n",
    "        ..., description=\"A brief, human-readable title for the cluster.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., description=\"A longer 1-2 sentence description of the cluster.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, I'll create some prompts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copyp of the conversation_cluster_df\n",
    "summarized_clusters_df = conversation_cluster_df.copy()\n",
    "\n",
    "# Add centroid summaries for each cluster\n",
    "summarized_clusters_df[\"centroid_summaries\"] = summarized_clusters_df[\n",
    "    \"centroid_conversation_ids\"\n",
    "].apply(\n",
    "    lambda ids: [\n",
    "        f\"**{title}**\\n\\n{summary}\"\n",
    "        for title, summary in summarized_conversations_df[\n",
    "            summarized_conversations_df[\"conversation_id\"].isin(ids)\n",
    "        ][[\"title\", \"summary\"]].values.tolist()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add a \"centroid_summary_markdown\" column for the AI to use\n",
    "summarized_clusters_df[\"centroid_summary_markdown\"] = summarized_clusters_df[\n",
    "    \"centroid_summaries\"\n",
    "].apply(lambda x: \"---\\n\\n\" + \"\\n\\n---\\n\\n\".join(x) + \"\\n\\n---\\n\\n\")\n",
    "\n",
    "# Create a \"prompt_markdown\" column for the AI to use, which contains both the centroid summary + the tag counts\n",
    "summarized_clusters_df[\"prompt_markdown\"] = summarized_clusters_df.apply(\n",
    "    lambda row: f\"# **Example Conversations:**\\n\\n{row['centroid_summary_markdown']}\\n\\n# **Tag Counts:**\\n\\n```json\\n{json.dumps(row['tag_counts'], indent=4)}\\n```\",\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the AI prompts look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(summarized_clusters_df.sample(1).iloc[0].prompt_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got these set up, I'm going to label them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the summary generation\n",
    "completions = openai_utils.generate_completions_in_parallel(\n",
    "    message_format_pairs=[\n",
    "        (\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_prompt}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": row.prompt_markdown,\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "            ConversationClusterSummary,\n",
    "        )\n",
    "        for row in summarized_clusters_df.itertuples()\n",
    "    ],\n",
    "    max_parallel_requests=24,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Construct a list of cluster summaries\n",
    "cluster_summaries = []\n",
    "for completion in completions:\n",
    "    try:\n",
    "        cluster_summaries.append(completion.choices[0].message.parsed)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing completion: {e}\")\n",
    "        print(completion)\n",
    "        cluster_summaries.append(None)\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "summarized_clusters_df[\"cluster_summary\"] = cluster_summaries\n",
    "summarized_clusters_df[\"cluster_title\"] = summarized_clusters_df[\n",
    "    \"cluster_summary\"\n",
    "].apply(lambda x: x.title if x else None)\n",
    "summarized_clusters_df[\"cluster_description\"] = summarized_clusters_df[\n",
    "    \"cluster_summary\"\n",
    "].apply(lambda x: x.description if x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of the clustered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_conversations_df.query(\"conversation_id=='86a40ac2-5669-4f2e-8f9f-8d5ce1c928e5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random cluster to display\n",
    "sample_cluster = summarized_clusters_df.sample(1).iloc[0]\n",
    "\n",
    "# Create a Markdown string that shows both the cluster title and description, and the prompt used for the AI\n",
    "markdown_str = f\"# **{sample_cluster.cluster_title}**\\n\\n*{sample_cluster.cluster_description}*\\n\\n{sample_cluster.prompt_markdown}\"\n",
    "\n",
    "display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
