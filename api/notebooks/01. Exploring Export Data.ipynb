{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploring Export Data**\n",
    "\n",
    "First thing's first: I want to actually explore the export data, and figure out how to run a pipeline for getting everything I need!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Third party imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "First, I'm going to load in the data. Since the export is a `.zip` file, I'll unzip it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to the export .zip file\n",
    "export_zip_path = \"./data/43f29f1d3bb293681d82b6ed415c1b2893d5ca12a4643f406316a0d1aaa1f3e8-2025-02-15-02-57-48-60b51c09ca924c7299b26640029988d7.zip\"\n",
    "\n",
    "# Declare the path where we'll extract the data\n",
    "export_path = \"./data/export\"\n",
    "\n",
    "# Extract the data from the export .zip file into a new folder\n",
    "# Using \\\\?\\ prefix to enable long paths on Windows\n",
    "extract_path = os.path.abspath(\"./data/export\")\n",
    "extract_path = \"\\\\\\\\?\\\\\" + extract_path\n",
    "\n",
    "with zipfile.ZipFile(export_zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright - after taking a look at things, here's a couple of things I've noticed:\n",
    "\n",
    "- `dalle-generations/` folder contains all of the DALL-E pictures you've generated\n",
    "- the main folder contains a TON of images - seems to be everything you've included in different chats\n",
    "- there are also a couple of folders in the main folder, each containing audio chats that I've had!\n",
    "- there's a huge `conversations.json` file that seems to contain... everything!\n",
    "- there's a `sora.json` file that contains pointers to different Sora videos I've made\n",
    "- they have a `message_feedback.json` and `model_comparisons.json`, too, which seems to contain some of the RLHF data I've given them. that's _really_ neat\n",
    "\n",
    "That's way more data than I've really expected. Since the platform changes so often, I'm sure that I can't rely _too_ heavily on the way things are structured, so... I'll try and write code keeping that in mind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding `conversations.json`\n",
    "\n",
    "To start out, I'm going to look into the `conversations.json`, since this is what I'm mainly interested in to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the conversations.json file\n",
    "conversations_path = os.path.join(extract_path, \"conversations.json\")\n",
    "with open(conversations_path, \"r\") as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the conversations data\n",
    "conversations_df = pd.DataFrame(conversations)\n",
    "\n",
    "# Print the length of the DataFrame\n",
    "print(f\"Loaded data for {len(conversations_df):,} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this data look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random conversation\n",
    "sample_conversation = conversations_df.sample(1).iloc[0]\n",
    "sample_conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the `mapping` contains the conversation itself. There are a couple of other fields that're interesting, like `plugin_ids`, `gizmo_type`, `default_model_slug`, and `conversation_origin`. I'll try and figure out what they are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field_of_interest in [\n",
    "    \"plugin_ids\",\n",
    "    \"gizmo_type\",\n",
    "    \"default_model_slug\",\n",
    "]:\n",
    "    print(f\"{conversations_df[field_of_interest].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After digging into `mapping` a bit more: seems like the conversation structure is a tree, which ultimately makes sense - ChatGPT, despite *seeming* like a linear chat, does have branching available via revising messages. \n",
    "\n",
    "Since I want to embed things within the OpenAI API, I want to be a bit lazy and create a \"canonical\" conversation. The easiest way to do this is to just extract the longest branch! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_conversation(mapping: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the longest \"chain\" of messages from a mapping of conversation nodes.\n",
    "\n",
    "    Args:\n",
    "        mapping (dict): A dictionary mapping node IDs to node information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the messages in the longest chain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to get chain from node to leaf\n",
    "    def get_chain_from_node(node_id):\n",
    "        chain = []\n",
    "        current_id = node_id\n",
    "\n",
    "        while current_id:\n",
    "            node = mapping[current_id]\n",
    "            if node[\"message\"] is not None:  # Only include non-null messages\n",
    "                chain.append(node[\"message\"])\n",
    "\n",
    "            # Move to child if exists, otherwise break\n",
    "            children = node[\"children\"]\n",
    "            current_id = children[0] if children else None\n",
    "\n",
    "        return chain\n",
    "\n",
    "    # Find all root nodes (nodes with no parent)\n",
    "    root_nodes = [\n",
    "        node_id for node_id, node in mapping.items() if node[\"parent\"] is None\n",
    "    ]\n",
    "\n",
    "    # Get all possible chains starting from root nodes\n",
    "    all_chains = [get_chain_from_node(root_id) for root_id in root_nodes]\n",
    "\n",
    "    # Get the longest chain\n",
    "    longest_chain = max(all_chains, key=len) if all_chains else []\n",
    "\n",
    "    # Create a DataFrame from the mapping\n",
    "    messages_df = pd.DataFrame(\n",
    "        sorted(\n",
    "            longest_chain,\n",
    "            key=lambda x: (x[\"create_time\"] or 0) if x else 0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add the \"author_role\" column\n",
    "    messages_df[\"author_role\"] = messages_df[\"author\"].apply(\n",
    "        lambda x: x[\"role\"] if x else None\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return messages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these DataFrames look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the longest conversation\n",
    "conversation_df = extract_longest_conversation(sample_conversation.mapping)\n",
    "conversation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
