{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Datamining Conversation Text**\n",
    "Within this notebook, I'm going to explore my options for organizing and datamining the conversation text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Project imports\n",
    "import utils.openai as openai_utils\n",
    "import utils.data_parsing as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "First off, I'll load in the conversations data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to the export data folder\n",
    "export_data_folder = \"./data/export\"\n",
    "\n",
    "# Read the conversations.json file\n",
    "conversations_path = f\"{export_data_folder}/conversations.json\"\n",
    "with open(conversations_path, \"r\") as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the conversations data\n",
    "conversations_df = pd.DataFrame(conversations)\n",
    "\n",
    "# Print the length of the DataFrame\n",
    "print(f\"Loaded data for {len(conversations_df):,} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Conversations\n",
    "For each of the conversations in the `conversations.json`, I'll get ChatGPT to generate a 1-2 sentence summary & a couple of \"tags\". \n",
    "\n",
    "I'll start by defining the system prompt & output format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt\n",
    "system_prompt = \"\"\"\n",
    "You're an intelligent AI assistant who likes responding in JSON. \n",
    "\n",
    "The user will provide you with a conversation between an AI chatbot and a user. \n",
    "Your task is to briefly - in 1-2 sentences - summarize the main topics covered within the conversation.\n",
    "You'll also provide a list of \"tags\" - these are keywords / short phrases that characterize the conversation.\n",
    "Tags ought to be lowercase, and relevant to the conversation content. Include anywhere between 3-5 tags. \n",
    "\"\"\"\n",
    "\n",
    "# Define a \"ConversationSummary\" Pydantic model that will describe the output format \n",
    "class ConversationSummary(BaseModel):\n",
    "    summary: str = Field(..., description=\"A 1-2 sentence summary of the conversation\")\n",
    "    tags: List[str] = Field(\n",
    "        ..., description=\"A list of tags that describe the conversation\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to generate the prompts for each of the conversations! I'll extract the longest chain of conversations from each tree (what I've deemed \"the canonical conversation\"), and put them each in a DataFrame. From there, I can make a prompt for each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the rows of the conversations DataFrame and extract the longest conversation\n",
    "simplified_conversation_df_records = []\n",
    "for convo_row in tqdm(\n",
    "    iterable=list(conversations_df.itertuples()),\n",
    "    desc=\"Extracting conversations from JSON data\",\n",
    "):\n",
    "\n",
    "    # Extract the longest conversation chain\n",
    "    longest_convo_chain_df = data_utils.extract_longest_conversation_df(\n",
    "        convo_row.mapping\n",
    "    )\n",
    "\n",
    "    # Filter out messages not from the assistant / user\n",
    "    filtered_convo_chain_df = longest_convo_chain_df.query(\n",
    "        \"author_role=='assistant' | author_role=='user'\"\n",
    "    )\n",
    "\n",
    "    # Add a record to the simplified conversation DataFrame\n",
    "    simplified_conversation_df_records.append(\n",
    "        {\n",
    "            \"conversation_id\": convo_row.conversation_id,\n",
    "            \"title\": convo_row.title,\n",
    "            \"raw_message_data\": filtered_convo_chain_df.to_dict(orient=\"records\"),\n",
    "            \"conversation_markdown\": data_utils.extract_simple_conversation_markdown(\n",
    "                filtered_convo_chain_df\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create a DataFrame from the simplified conversation records\n",
    "simplified_conversation_df = pd.DataFrame(simplified_conversation_df_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this DataFrame look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample of the DataFrame\n",
    "simplified_conversation_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the `conversation_markdown` look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random conversation from the DataFrame and show the Markdown representation\n",
    "display(\n",
    "    Markdown(simplified_conversation_df.sample(1).iloc[0].conversation_markdown[:2000])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that I've got all of these: I can create the summaries! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the summary generation\n",
    "MAX_CHARS_PER_SUMMARY = 4_000\n",
    "\n",
    "completions = openai_utils.generate_completions_in_parallel(\n",
    "    message_format_pairs=[\n",
    "        (\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"developer\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_prompt}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": row.conversation_markdown[:MAX_CHARS_PER_SUMMARY]\n",
    "                            + \"...\",\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "            ConversationSummary,\n",
    "        )\n",
    "        for row in simplified_conversation_df.itertuples()\n",
    "    ],\n",
    "    max_parallel_requests=24,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Construct a list of conversation summaries\n",
    "conversation_summaries = []\n",
    "for completion in completions:\n",
    "    try:\n",
    "        conversation_summaries.append(completion.choices[0].message.parsed)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing completion: {e}\")\n",
    "        print(completion)\n",
    "        conversation_summaries.append(None)\n",
    "\n",
    "# Make a new DataFrame from the conversation summaries\n",
    "summarized_conversations_df = simplified_conversation_df.copy()\n",
    "summarized_conversations_df[\"conversation_summary\"] = conversation_summaries\n",
    "\n",
    "# Add the summary and tags to the DataFrame\n",
    "summarized_conversations_df[\"summary\"] = summarized_conversations_df[\n",
    "    \"conversation_summary\"\n",
    "].apply(lambda x: x.summary if x else None)\n",
    "summarized_conversations_df[\"tags\"] = summarized_conversations_df[\n",
    "    \"conversation_summary\"\n",
    "].apply(lambda x: x.tags if x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the field names & values for a sample conversation\n",
    "sample_conversation = summarized_conversations_df.sample(1).iloc[0]\n",
    "\n",
    "display(\n",
    "    Markdown(data_utils.extract_summarized_conversation_markdown(sample_conversation))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving Data**\n",
    "Below, I'm going to save the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the folder path for the export data\n",
    "parsed_export_data = f\"./data/parsed_export\"\n",
    "Path(parsed_export_data).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save a .json file with the summarized conversations\n",
    "summarized_conversations_df.to_json(\n",
    "    f\"{parsed_export_data}/summarized_conversations.json\", orient=\"records\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Data**\n",
    "If I'm reloading this notebook later on, I can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the folder path for the export data\n",
    "parsed_export_data = f\"./data/parsed_export\"\n",
    "\n",
    "# Load the summarized conversations DataFrame\n",
    "summarized_conversations_df = pd.read_json(\n",
    "    f\"{parsed_export_data}/summarized_conversations.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Conversations\n",
    "Next up, I'm going to run these conversations through OpenAI's embedding models. That way, I'll get some embeddings for each conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the conversation embeddings\n",
    "conversation_embs_df = summarized_conversations_df.copy()\n",
    "\n",
    "# Generate embeddings for the conversations\n",
    "embs = openai_utils.generate_embeddings_for_texts(\n",
    "    text_list=[\n",
    "        f\"{row.title}\\nTags: {', '.join(row.tags)}\\nSummary: {row.summary}\"\n",
    "        for row in conversation_embs_df.itertuples()\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Add the embeddings to the DataFrame\n",
    "conversation_embs_df[\"embedding\"] = embs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving Data**\n",
    "Below, I'll save some of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the folder path for the export data\n",
    "parsed_export_data = f\"./data/parsed_export\"\n",
    "\n",
    "# Save the conversation embeddings to a .parquet file\n",
    "conversation_embs_df[[\"conversation_id\", \"embedding\"]].to_parquet(\n",
    "    f\"{parsed_export_data}/conversation_embeddings.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Data**\n",
    "Next, I'll reload the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the folder path for the export data\n",
    "parsed_export_data = f\"./data/parsed_export\"\n",
    "\n",
    "# Load the conversation embeddings DataFrame\n",
    "conversation_embs_df = pd.read_parquet(\n",
    "    f\"{parsed_export_data}/conversation_embeddings.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
